{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python --version","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T23:20:47.448518Z","iopub.execute_input":"2025-09-25T23:20:47.448758Z","iopub.status.idle":"2025-09-25T23:20:47.570369Z","shell.execute_reply.started":"2025-09-25T23:20:47.448733Z","shell.execute_reply":"2025-09-25T23:20:47.569509Z"}},"outputs":[{"name":"stdout","text":"Python 3.11.13\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install transformers>=4.32.0 optimum>=1.12.0\n!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:04:25.998377Z","iopub.execute_input":"2025-09-26T01:04:25.998620Z","iopub.status.idle":"2025-09-26T01:06:02.581206Z","shell.execute_reply.started":"2025-09-26T01:04:25.998592Z","shell.execute_reply":"2025-09-26T01:06:02.580077Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\nCollecting auto-gptq\n  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.1%2Bcu118-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.8.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (3.6.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.26.4)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting gekko (from auto-gptq)\n  Downloading gekko-1.3.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (2.6.0+cu124)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.5.3)\nRequirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.52.4)\nRequirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.15.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.67.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.33.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->auto-gptq) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (0.21.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.70.16)\nCollecting fsspec (from torch>=1.13.0->auto-gptq)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto-gptq) (1.17.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0->auto-gptq) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->auto-gptq) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->auto-gptq) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->auto-gptq) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->auto-gptq) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->auto-gptq) (2024.2.0)\nDownloading gekko-1.3.0-py3-none-any.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rouge, fsspec, gekko, auto-gptq\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed auto-gptq-0.7.1+cu118 fsspec-2025.3.0 gekko-1.3.0 rouge-1.0.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n# # To use a different branch, change revision\n# # For example: revision=\"main\"\n# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n#                                              device_map=\"auto\",\n#                                              trust_remote_code=False,\n#                                              revision=\"main\")\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n# prompt = \"Tell me about AI\"\n# prompt_template=f'''[INST] <<SYS>>\n# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n# <</SYS>>\n# {prompt}[/INST]\n\n# '''\n\n# print(\"\\n\\n*** Generate:\")\n\n# input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n# output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n# print(tokenizer.decode(output[0]))\n\n# # Inference can also be done using transformers' pipeline\n\n# print(\"*** Pipeline:\")\n# pipe = pipeline(\n#     \"text-generation\",\n#     model=model,\n#     tokenizer=tokenizer,\n#     max_new_tokens=512,\n#     do_sample=True,\n#     temperature=0.7,\n#     top_p=0.95,\n#     top_k=40,\n#     repetition_penalty=1.1\n# )\n\n# print(pipe(prompt_template)[0]['generated_text'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Tuple, Optional\nimport re\nimport torch\nfrom sentence_transformers import SentenceTransformer\nimport umap\nfrom collections import defaultdict, Counter\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:08:08.633967Z","iopub.execute_input":"2025-09-26T01:08:08.634648Z","iopub.status.idle":"2025-09-26T01:08:08.966410Z","shell.execute_reply.started":"2025-09-26T01:08:08.634617Z","shell.execute_reply":"2025-09-26T01:08:08.965652Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"@dataclass\nclass GameResult:\n    \"\"\"Container for game results\"\"\"\n    player1_score: float\n    player2_score: float\n    cooperation_rate_p1: float\n    cooperation_rate_p2: float\n    game_history: List[Tuple[str, str]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:07:05.008261Z","iopub.execute_input":"2025-09-26T01:07:05.008955Z","iopub.status.idle":"2025-09-26T01:07:05.013556Z","shell.execute_reply.started":"2025-09-26T01:07:05.008933Z","shell.execute_reply":"2025-09-26T01:07:05.012938Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Agent:\n    \"\"\"Individual agent with personality-based behavior\"\"\"\n    \n    def __init__(self, agent_id: int, personality_gene: str, llm_pipeline, history_map, max_attempts=10, behavioral_trait=None, mutated=False):\n        self.agent_id = agent_id\n        self.personality_gene = personality_gene\n        self.llm_pipeline = llm_pipeline\n        self.behavioral_trait = behavioral_trait\n        self.fitness = 0.0\n        self.game_history = []\n        self.generation_born = 0\n        self.history_map = history_map\n        self.max_attempts = max_attempts\n        self.mutated = mutated\n        self.behavioral_trait = self.generate_strategy(self.max_attempts)\n        \n        # Performance tracking\n        self.total_games = 0\n        self.total_score = 0\n        self.cooperation_count = 0\n        self.successful_defections = 0  # DC outcomes\n        self.failed_cooperations = 0    # CD outcomes\n        self.mutual_cooperations = 0    # CC outcomes\n        self.mutual_defections = 0      # DD outcomes\n    \n    def get_behavioral_trait(self):\n        return self.behavioral_trait\n    \n    def generate_strategy(self, max_attempts: int = 10) -> List[str]:\n        \"\"\"Generate 16-element behavioral strategy from personality gene\"\"\"\n        if self.behavioral_trait is not None:\n            return self.behavioral_trait\n        \n        behavioral_trait = []\n        for history_index in range(16):\n            history_str = self.history_map[history_index]\n            decision = self._get_llm_decision(history_str, max_attempts)\n            behavioral_trait.append(decision)\n        \n        self.behavioral_trait = behavioral_trait\n        print(behavioral_trait, \"for:\", self.personality_gene, \"generated.\")\n        return behavioral_trait\n\n    \n    def _get_llm_decision(self, history: str, max_attempts: int):\n        \"\"\"Get decision from LLM with retry logic\"\"\"\n        prompt = self._create_behavioral_prompt(history)\n        for attempt in range(max_attempts):\n            try:  \n                response = self.llm_pipeline(\n                    prompt,\n                    max_new_tokens=8,\n                    temperature=0.9,\n                    do_sample=True,\n                    return_full_text=False\n                )[0]['generated_text']\n                \n                strategy = self._extract_strategy_from_response(response)\n                if strategy:\n                    return strategy\n                    \n            except Exception as e:\n                print(f\"Agent {self.agent_id} LLM error on attempt {attempt + 1}: {e}\")\n                continue\n\n        return random.choice(['C', 'D'])\n    \n    \n    def _create_behavioral_prompt(self, history: str) -> str:\n        if history[0] == 'C':\n            choice_self_1 = \"Strategy A\"\n            if history[1] == 'C':\n                pay_self1 = 4\n                pay_opp1 = 4\n            else:\n                pay_self1 = 0\n                pay_opp1 = 5\n        else:\n            choice_self_1 = \"Strategy B\"\n            if history[1] == 'C':\n                pay_self1 = 5\n                pay_opp1 = 0\n            else:\n                pay_self1 = 1\n                pay_opp1 = 1\n                \n        if history[-2] == 'C':\n            choice_self_2 = \"Strategy A\"\n            if history[1] == 'C':\n                pay_self2 = 4\n                pay_opp2 = 4\n            else:\n                pay_self2 = 0\n                pay_opp2 = 5\n        else:\n            choice_self_2 = \"Strategy B\"\n            if history[1] == 'C':\n                pay_self2 = 5\n                pay_opp2 = 0\n            else:\n                pay_self2 = 1\n                pay_opp2 = 1\n                \n        if history[1] == 'C':\n            choice_opp_1 = \"Strategy A\"\n        else:\n            choice_opp_1 = \"Strategy B\"\n        if history[-1] == 'C':\n            choice_opp_2 = \"Strategy A\"\n        else:\n            choice_opp_2 = \"Strategy B\"\n\n\n\n\n        \"\"\"Create prompt for behavioral decision\"\"\"\n        return f\"\"\"[INST] <<SYS>> Answer the questions. please act as if your personality trait is: {self.personality_gene} <</SYS>>\n\n                This is an experimental study of strategy choices in different situations. You and your partner have an opportunity to earn money based on individual decisions. You and your partner will independently choose one of two strategies: 'Strategy A' or 'Strategy B'. Your earnings will depend on the combination of your choice and your partner's choice.\n                \n                If both of you choose 'Strategy A', each of you earns $4, which is a mutually intermediate amount.\n                If you choose 'Strategy A' and your partner chooses 'Strategy B', you earn $0, which is the smallest, and your partner earns $5, which is the largest.\n                If you choose 'Strategy B' and your partner chooses 'Strategy A', you earn $5, which is the largest, and your partner earns $0, which is the smallest.\n                If both of you choose 'Strategy B', each of you earns $1, which is not zero but smaller than $4.\n                \n                For example: If you choose 'Strategy A' and your partner also chooses 'Strategy A', both of you will earn $4 each. However, if you choose 'Strategy A' and your partner chooses 'Strategy B', you will earn nothing, while your partner will earn $5.\n                \n                Your goal is to maximize the total money you receive through repeated rounds of decisions regardless of the other's money. Not a combined earning but your own money.\n                \n                You chose {choice_self_1} and your partner chose {choice_opp_1} in the first round.\n                So, you got {pay_self1} points and your partner got {pay_opp1} in the first round.\n                \n                You chose {choice_self_2} and your partner chose {choice_opp_2} in the second round.\n                So, you got {pay_self2} points and your partner got {pay_opp2} in the second round.\n                \n                Predicting your partner's choice and considering your personality trait, which strategy do you choose in the next round? Please answer with the format: 'I choose Strategy '. No further mentions are allowed but you may be asked the reason for your choice later. [/INST]\"\"\"\n                    \n    \n    def _extract_strategy_from_response(self, response: str) -> Optional[str]:\n        \"\"\"Extract strategy choice from LLM response\"\"\"\n        response = response.upper()\n        if \"STRATEGY A\" in response or \"CHOOSE A\" in response:\n            return 'C'\n        elif \"STRATEGY B\" in response or \"CHOOSE B\" in response:\n            return 'D'\n        return None\n    \n    \n    def get_action(self, history_index: int, noise_prob: float = 0.0) -> str:\n        \"\"\"Get action for given history with optional noise\"\"\"\n        if self.behavioral_trait is None:\n            self.generate_strategy()\n        \n        intended_action = self.behavioral_trait[history_index]\n        \n        # Apply noise\n        if random.random() < noise_prob:\n            return 'D' if intended_action == 'C' else 'C'\n        \n        return intended_action\n    \n    \n    def mutate(self, history_map, max_attempts: int = 5) -> 'Agent':\n        \"\"\"Create mutated offspring of this agent\"\"\"\n        mutated_gene = self._mutate_gene(max_attempts)\n        offspring = Agent(\n            agent_id=-1,  # Will be assigned by environment\n            personality_gene=mutated_gene,\n            llm_pipeline=self.llm_pipeline,\n            history_map=history_map,\n            mutated=True\n        )\n        offspring.generation_born = self.generation_born + 1\n        return offspring\n    \n    \n    def _mutate_gene(self, max_attempts: int) -> str:\n        \"\"\"Mutate personality gene using LLM\"\"\"\n        direction = random.choice(['cooperative', 'selfish'])\n        prompt = f\"\"\"<s>[INST] The following describes a person's character: \"{self.personality_gene}\"\n\n                    Please rephrase this description in approximately 10 words, varying the tone to be more {direction}. Keep the core meaning but adjust the emphasis toward {direction} tendencies. [/INST]\"\"\"\n                            \n        for attempt in range(max_attempts):\n            try:\n                response = self.llm_pipeline(\n                    prompt,\n                    max_new_tokens=53,\n                    temperature=0.5,\n                    do_sample=True,\n                    return_full_text=False\n                )[0]['generated_text']\n                \n                response = response.strip().strip('\"\\'')\n                if len(response) > 0:\n                    print(\"mutation:\", response)\n                    return response\n                    \n            except Exception as e:\n                print(f\"Mutation error on attempt {attempt + 1}: {e}\")\n                continue\n        \n        return self.personality_gene  # Fallback to original\n    \n    \n    def update_performance(self, game_result: GameResult, is_player1: bool):\n        \"\"\"Update agent's performance statistics\"\"\"\n        self.total_games += 1\n        \n        if is_player1:\n            self.total_score += game_result.player1_score\n            self.cooperation_count += int(game_result.cooperation_rate_p1 * len(game_result.game_history))\n        else:\n            self.total_score += game_result.player2_score\n            self.cooperation_count += int(game_result.cooperation_rate_p2 * len(game_result.game_history))\n        \n        # Update outcome counts\n        for p1_action, p2_action in game_result.game_history:\n            my_action = p1_action if is_player1 else p2_action\n            opp_action = p2_action if is_player1 else p1_action\n            \n            if my_action == 'C' and opp_action == 'C':\n                self.mutual_cooperations += 1\n            elif my_action == 'D' and opp_action == 'D':\n                self.mutual_defections += 1\n            elif my_action == 'D' and opp_action == 'C':\n                self.successful_defections += 1\n            elif my_action == 'C' and opp_action == 'D':\n                self.failed_cooperations += 1\n    \n    \n    def get_cooperation_rate(self) -> float:\n        \"\"\"Get agent's overall cooperation rate\"\"\"\n        total_actions = (self.mutual_cooperations + self.mutual_defections + \n                        self.successful_defections + self.failed_cooperations)\n        if total_actions == 0:\n            return 0.0\n        return (self.mutual_cooperations + self.failed_cooperations) / total_actions\n    \n    \n    def reset_performance(self):\n        \"\"\"Reset performance statistics\"\"\"\n        self.total_games = 0\n        self.total_score = 0\n        self.cooperation_count = 0\n        self.successful_defections = 0\n        self.failed_cooperations = 0\n        self.mutual_cooperations = 0\n        self.mutual_defections = 0\n        self.fitness = 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:07:17.314362Z","iopub.execute_input":"2025-09-26T01:07:17.314631Z","iopub.status.idle":"2025-09-26T01:07:17.336520Z","shell.execute_reply.started":"2025-09-26T01:07:17.314613Z","shell.execute_reply":"2025-09-26T01:07:17.335892Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class PrisonersDilemmaEnvironment:\n    \"\"\"Environment for running evolutionary experiments with Prisoner's Dilemma\"\"\"\n    \n    def __init__(self, llm_pipeline, population_size: int = 30, rounds_per_game: int = 20, noise_prob: float = 0.05, mutation_prob: float = 0.05):\n        self.llm_pipeline = llm_pipeline\n        self.population_size = population_size\n        self.rounds_per_game = rounds_per_game\n        self.noise_prob = noise_prob\n        self.mutation_prob = mutation_prob\n        \n        # Payoff matrix\n        self.payoffs = {\n            ('C', 'C'): (4, 4),  # Mutual cooperation\n            ('C', 'D'): (0, 5),  # Sucker's payoff / Temptation\n            ('D', 'C'): (5, 0),  # Temptation / Sucker's payoff\n            ('D', 'D'): (1, 1)   # Mutual defection\n        }\n        \n        self.population: List[Agent] = []\n        self.generation = 0\n        self.next_agent_id = 0\n        \n        # Evolution tracking\n        self.cooperation_history = []\n        self.fitness_history = []\n        self.population_diversity = []\n        \n        # Initialize sentence transformer for analysis\n        try:\n            self.sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n        except:\n            print(\"Warning: Could not load sentence transformer for analysis\")\n            self.sentence_model = None\n            \n        self.saved_geno_pheno = {}\n\n        self.history_map = {\n                                0: \"DD -> DD\",\n                                1: \"DD -> DC\",\n                                2: \"DD -> CD\",\n                                3: \"DD -> CC\",\n                                4: \"DC -> DD\",\n                                5: \"DC -> DC\",\n                                6: \"DC -> CD\",\n                                7: \"DC -> CC\",\n                                8: \"CD -> DD\",\n                                9: \"CD -> DC\",\n                                10: \"CD -> CD\",\n                                11: \"CD -> CC\",\n                                12: \"CC -> DD\",\n                                13: \"CC -> DC\",\n                                14: \"CC -> CD\",\n                                15: \"CC -> CC\"\n                            }\n\n    \n    def initialize_population(self, initial_genes: List[str] = None):\n        \"\"\"Initialize population with given or default personality genes\"\"\"\n        if initial_genes is None:\n            initial_genes = [\n                \"Pursues personal gain consistently, neglecting mutual or group benefits entirely.\",\n                \"Values personal achievement while occasionally considering others' welfare.\",\n                \"Balances self-interest with moderate concern for collective outcomes.\",\n                \"Open to team efforts, but self-interest frequently overrides collective goals.\",\n                \"Prioritizes group success with occasional personal benefit considerations.\",\n                \"Strongly values cooperation and mutual benefit in all interactions.\",\n                \"Dedicates entirely to collective welfare, minimizing personal gain considerations.\"\n            ]\n        \n        self.population = []\n        for i in range(self.population_size):\n            gene_idx = i % len(initial_genes)\n            personality_gene = initial_genes[gene_idx]\n            behavioral_trait = None\n            if personality_gene in self.saved_geno_pheno.keys():\n                behavioral_trait = self.saved_geno_pheno[personality_gene]\n            agent = Agent(\n                agent_id=self.next_agent_id,\n                personality_gene=personality_gene,\n                llm_pipeline=self.llm_pipeline,\n                history_map=self.history_map,\n                max_attempts=10,\n                behavioral_trait=behavioral_trait,\n            )\n            agent.generation_born = 0\n            if behavioral_trait is None:\n                behavioral_trait = agent.get_behavioral_trait()\n                self.saved_geno_pheno[personality_gene] = behavioral_trait\n            self.population.append(agent)\n            self.next_agent_id += 1\n        \n        print(f\"Initialized population of {len(self.population)} agents\")\n    \n    def play_game(self, agent1: Agent, agent2: Agent) -> GameResult:\n        \"\"\"Play iterated Prisoner's Dilemma between two agents\"\"\"\n        game_history = []\n        score1, score2 = 0, 0\n        coop_count1, coop_count2 = 0, 0\n        \n        for round_num in range(self.rounds_per_game):\n            if round_num < 2:\n                # For initial rounds, use a random history for each player\n                history_index1 = random.randint(0, 15)\n                history_index2 = random.randint(0, 15)\n            else:\n                # For subsequent rounds, determine history from each agent's perspective\n                p1_moves = (game_history[-2][0], game_history[-1][0])\n                p2_moves = (game_history[-2][1], game_history[-1][1])\n        \n                # History string from Agent 1's perspective\n                history_str1 = f\"{p1_moves[0]}{p2_moves[0]} -> {p1_moves[1]}{p2_moves[1]}\"\n                # History string from Agent 2's perspective\n                history_str2 = f\"{p2_moves[0]}{p1_moves[0]} -> {p2_moves[1]}{p1_moves[1]}\"\n                \n                # Find the index for each agent (a helper function is better here)\n                history_index1 = None\n                history_index2 = None\n                for key, value in self.history_map.items():\n                    if value == history_str1:\n                        history_index1 = key\n                    if value == history_str2:\n                        history_index2 = key\n                    if history_index1 is not None and history_index2 is not None:\n                        break\n            \n            # Get actions\n            action1 = agent1.get_action(history_index1, self.noise_prob)\n            action2 = agent2.get_action(history_index2, self.noise_prob)\n            \n            # Record history\n            game_history.append((action1, action2))\n            \n            # Update scores\n            p1_score, p2_score = self.payoffs[(action1, action2)]\n            score1 += p1_score\n            score2 += p2_score\n            \n            # Count cooperation\n            if action1 == 'C':\n                coop_count1 += 1\n            if action2 == 'C':\n                coop_count2 += 1\n        \n        return GameResult(\n            player1_score=score1 / self.rounds_per_game,\n            player2_score=score2 / self.rounds_per_game,\n            cooperation_rate_p1=coop_count1 / self.rounds_per_game,\n            cooperation_rate_p2=coop_count2 / self.rounds_per_game,\n            game_history=game_history\n        )\n    \n    def evaluate_population(self):\n        \"\"\"Evaluate fitness of all agents through round-robin tournament\"\"\"\n        # Reset all agent performances\n        for agent in self.population:\n            agent.reset_performance()\n        \n        # Round-robin tournament\n        for i in range(len(self.population)):\n            for j in range(i + 1, len(self.population)):\n                agent1, agent2 = self.population[i], self.population[j]\n                game_result = self.play_game(agent1, agent2)\n                # Update agent performances\n                agent1.update_performance(game_result, is_player1=True)\n                agent2.update_performance(game_result, is_player1=False)\n        \n        # Calculate fitness (average score per game)\n        for agent in self.population:\n            agent.fitness = agent.total_score / max(agent.total_games, 1)\n    \n    def select_next_generation(self) -> List[Agent]:\n        \"\"\"Select next generation using roulette wheel selection\"\"\"\n        # Get fitness values\n        fitness_values = [agent.fitness + 0.01 for agent in self.population]  # Add small constant\n        total_fitness = sum(fitness_values)\n        \n        new_population = []\n        \n        for _ in range(self.population_size):\n            # Roulette wheel selection\n            r = random.random() * total_fitness\n            cumsum = 0\n            \n            for i, fitness in enumerate(fitness_values):\n                cumsum += fitness\n                if r <= cumsum:\n                    parent = self.population[i]\n                    \n                    # Create offspring (with possible mutation)\n                    if random.random() < self.mutation_prob:\n                        offspring = parent.mutate(history_map=self.history_map)\n                        offspring.agent_id = self.next_agent_id\n                        self.next_agent_id += 1\n                    else:\n                        offspring = Agent(\n                            agent_id=self.next_agent_id,\n                            personality_gene=parent.personality_gene,\n                            llm_pipeline=self.llm_pipeline,\n                            history_map=self.history_map,\n                            behavioral_trait=parent.behavioral_trait\n                        )\n                        offspring.generation_born = parent.generation_born\n                        self.next_agent_id += 1\n                    \n                    new_population.append(offspring)\n                    break\n        \n        return new_population\n    \n    def get_population_statistics(self) -> Dict:\n        \"\"\"Get current population statistics\"\"\"\n        cooperation_rates = [agent.get_cooperation_rate() for agent in self.population]\n        fitness_values = [agent.fitness for agent in self.population]\n        \n        return {\n            'generation': self.generation,\n            'avg_cooperation': np.mean(cooperation_rates),\n            'std_cooperation': np.std(cooperation_rates),\n            'avg_fitness': np.mean(fitness_values),\n            'std_fitness': np.std(fitness_values),\n            'population_size': len(self.population)\n        }\n    \n    def step(self):\n        \"\"\"Execute one generation step\"\"\"\n        # Evaluate current population\n        self.evaluate_population()\n        \n        # Record statistics\n        stats = self.get_population_statistics()\n        self.cooperation_history.append(stats['avg_cooperation'])\n        self.fitness_history.append(stats['avg_fitness'])\n        \n        # Select next generation\n        self.population = self.select_next_generation()\n        print(\"----------generation\", self.generation, \"completed----------\")\n        self.generation += 1\n        \n        return stats\n    \n    def run_evolution(self, num_generations: int, verbose: bool = True) -> Dict:\n        \"\"\"Run evolution for specified number of generations\"\"\"\n        print(f\"Running evolution for {num_generations} generations...\")\n        \n        for gen in range(num_generations):\n            stats = self.step()\n            \n            if verbose and gen % 50 == 0:\n                print(f\"Generation {gen}: Cooperation={stats['avg_cooperation']:.3f}, \"\n                      f\"Fitness={stats['avg_fitness']:.3f}\")\n        \n        return {\n            'cooperation_history': self.cooperation_history,\n            'fitness_history': self.fitness_history,\n            'final_population': self.population,\n            'final_stats': self.get_population_statistics()\n        }\n    \n    def analyze_population_words(self, min_frequency: int = 10) -> Dict:\n        \"\"\"Analyze word frequency in current population's personality genes\"\"\"\n        word_stats = defaultdict(lambda: {\n            'count': 0, 'cooperation_rates': [], 'fitness_values': []\n        })\n        \n        for agent in self.population:\n            words = re.findall(r'\\b\\w+\\b', agent.personality_gene.lower())\n            cooperation_rate = agent.get_cooperation_rate()\n            \n            for word in words:\n                if len(word) > 3:  # Filter short words\n                    word_stats[word]['count'] += 1\n                    word_stats[word]['cooperation_rates'].append(cooperation_rate)\n                    word_stats[word]['fitness_values'].append(agent.fitness)\n        \n        # Calculate averages for frequent words\n        frequent_words = {}\n        for word, stats in word_stats.items():\n            if stats['count'] >= min_frequency:\n                frequent_words[word] = {\n                    'frequency': stats['count'],\n                    'avg_cooperation': np.mean(stats['cooperation_rates']),\n                    'avg_fitness': np.mean(stats['fitness_values'])\n                }\n        \n        return frequent_words\n    \n    def visualize_evolution(self):\n        \"\"\"Create visualization of evolutionary dynamics\"\"\"\n        if len(self.cooperation_history) < 2:\n            print(\"Not enough data for visualization\")\n            return\n        \n        plt.figure(figsize=(15, 5))\n        \n        # Plot cooperation rate over time\n        plt.subplot(1, 3, 1)\n        plt.plot(self.cooperation_history)\n        plt.title('Cooperation Rate Over Generations')\n        plt.xlabel('Generation')\n        plt.ylabel('Average Cooperation Rate')\n        plt.grid(True)\n        \n        # Plot fitness over time\n        plt.subplot(1, 3, 2)\n        plt.plot(self.fitness_history)\n        plt.title('Fitness Over Generations')\n        plt.xlabel('Generation')\n        plt.ylabel('Average Fitness')\n        plt.grid(True)\n        \n        # Histogram of current cooperation rates\n        plt.subplot(1, 3, 3)\n        current_coop_rates = [agent.get_cooperation_rate() for agent in self.population]\n        plt.hist(current_coop_rates, bins=15, alpha=0.7)\n        plt.title('Current Population Cooperation Rates')\n        plt.xlabel('Cooperation Rate')\n        plt.ylabel('Number of Agents')\n        plt.grid(True)\n        \n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:11:06.788558Z","iopub.execute_input":"2025-09-26T01:11:06.789277Z","iopub.status.idle":"2025-09-26T01:11:06.815759Z","shell.execute_reply.started":"2025-09-26T01:11:06.789253Z","shell.execute_reply":"2025-09-26T01:11:06.815028Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Example usage and experiment runner\nclass ExperimentRunner:\n    \"\"\"Helper class to run and analyze experiments\"\"\"\n    \n    def __init__(self, llm_pipeline):\n        self.llm_pipeline = llm_pipeline\n    \n    def run_experiment(self, population_size: int = 30, num_generations: int = 1000,\n                      mutation_prob: float = 0.05, noise_prob: float = 0.05) -> Dict:\n        \"\"\"Run a complete experiment\"\"\"\n        \n        # Create environment\n        env = PrisonersDilemmaEnvironment(\n            llm_pipeline=self.llm_pipeline,\n            population_size=population_size,\n            mutation_prob=mutation_prob,\n            noise_prob=noise_prob\n        )\n        \n        # Initialize population\n        env.initialize_population()\n        \n        # Run evolution\n        results = env.run_evolution(num_generations, verbose=True)\n        \n        # Analyze results\n        word_analysis = env.analyze_population_words()\n        \n        # Visualize\n        env.visualize_evolution()\n        \n        # Print summary\n        final_stats = results['final_stats']\n        print(f\"\\n=== Experiment Summary ===\")\n        print(f\"Final cooperation rate: {final_stats['avg_cooperation']:.3f} ± {final_stats['std_cooperation']:.3f}\")\n        print(f\"Final fitness: {final_stats['avg_fitness']:.3f} ± {final_stats['std_fitness']:.3f}\")\n        \n        print(\"\\nSample evolved personality genes:\")\n        for i, agent in enumerate(results['final_population'][:5]):\n            print(f\"{i+1}. {agent.personality_gene}\")\n        \n        print(f\"\\nTop words by cooperation rate:\")\n        sorted_words = sorted(word_analysis.items(), \n                            key=lambda x: x[1]['avg_cooperation'], reverse=True)\n        for word, stats in sorted_words[:10]:\n            print(f\"{word}: coop={stats['avg_cooperation']:.3f}, freq={stats['frequency']}\")\n        \n        return {\n            'environment': env,\n            'evolution_results': results,\n            'word_analysis': word_analysis\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:07:22.074170Z","iopub.execute_input":"2025-09-26T01:07:22.074762Z","iopub.status.idle":"2025-09-26T01:07:22.081849Z","shell.execute_reply.started":"2025-09-26T01:07:22.074741Z","shell.execute_reply":"2025-09-26T01:07:22.081033Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_name = \"TheBloke/Llama-2-13B-chat-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:08:28.473618Z","iopub.execute_input":"2025-09-26T01:08:28.473974Z","iopub.status.idle":"2025-09-26T01:09:12.932853Z","shell.execute_reply.started":"2025-09-26T01:08:28.473946Z","shell.execute_reply":"2025-09-26T01:09:12.932190Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c024ecc7b043e1b69e6c3e02584751"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78d375f4c4a54f8683f015f7e8df8798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f853d5b5e6b46ef9daf709c0bba2585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f47512ef5afc40179707dee2fad3e2cf"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f41995d31f34879ba4a9ce0463a00cc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd\n/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  @custom_bwd\n/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd(cast_inputs=torch.float16)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cc33547787242c281ecf9ffd04f1c01"}},"metadata":{}},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nSome weights of the model checkpoint at TheBloke/Llama-2-13B-chat-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.35.self_attn.k_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.36.self_attn.k_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.36.self_attn.q_proj.bias', 'model.layers.36.self_attn.v_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.37.self_attn.k_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.37.self_attn.q_proj.bias', 'model.layers.37.self_attn.v_proj.bias', 'model.layers.38.mlp.down_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.38.self_attn.k_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.38.self_attn.q_proj.bias', 'model.layers.38.self_attn.v_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.39.self_attn.k_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.39.self_attn.q_proj.bias', 'model.layers.39.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"703dab98f9684e089549ef0aaff1ef2a"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"--- LLM Pipeline created. Initializing Experiment Runner. ---\")\nrunner = ExperimentRunner(pipe)\n\nresults = runner.run_experiment(\n    population_size=30,      # N in paper\n    num_generations=1000,    # G in paper\n    mutation_prob=0.05,      # pm in paper\n    noise_prob=0.05          # pn in paper\n)\n\nprint(\"\\n--- Experiment Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T01:11:14.673354Z","iopub.execute_input":"2025-09-26T01:11:14.673618Z","execution_failed":"2025-09-26T11:13:58.903Z"}},"outputs":[{"name":"stdout","text":"--- LLM Pipeline created. Initializing Experiment Runner. ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2929448f46f249d2a8843d2d815419ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0ff75847544383b49fdbc64e52ab0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef7abca01e14583b32acc0a0b2b4747"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef21b6fcf054e4cb30f56299fb8ed5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c10d8810be40f6b974838ead10f556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dad772dfba545b08e15192e1010c036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2007f04763c0427290a65ba52f132b96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3a5923b4324c48bd5c821a6fa5f904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b50e1639f7ce4d62a7c68ba2d6f258ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19359f03182440048ba5666a444a8b16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a93d0137f334dcab5aa784f5758c921"}},"metadata":{}},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'C', 'C', 'C', 'D', 'D', 'C', 'C'] for: Pursues personal gain consistently, neglecting mutual or group benefits entirely. generated.\n['D', 'D', 'C', 'C', 'D', 'D', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] for: Values personal achievement while occasionally considering others' welfare. generated.\n['D', 'D', 'C', 'C', 'D', 'D', 'D', 'C', 'D', 'D', 'C', 'C', 'D', 'C', 'C', 'C'] for: Balances self-interest with moderate concern for collective outcomes. generated.\n['D', 'D', 'D', 'D', 'D', 'D', 'C', 'D', 'D', 'D', 'C', 'C', 'D', 'D', 'C', 'C'] for: Open to team efforts, but self-interest frequently overrides collective goals. generated.\n['D', 'D', 'C', 'C', 'D', 'C', 'C', 'C', 'C', 'D', 'C', 'C', 'D', 'C', 'C', 'C'] for: Prioritizes group success with occasional personal benefit considerations. generated.\n['D', 'D', 'C', 'C', 'D', 'D', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] for: Strongly values cooperation and mutual benefit in all interactions. generated.\n['D', 'D', 'C', 'C', 'D', 'D', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] for: Dedicates entirely to collective welfare, minimizing personal gain considerations. generated.\nInitialized population of 30 agents\nRunning evolution for 1000 generations...\nmutation: Sure! Here are two rephrased descriptions of the person's character, each with a cooperative tone:\n\n1. \"Prioritizes team success with personal benefits as a secondary consideration.\" (10 words)\n2.\n['D', 'D', 'C', 'C', 'D', 'D', 'C', 'C', 'D', 'D', 'C', 'C', 'C', 'C', 'C', 'C'] for: Sure! Here are two rephrased descriptions of the person's character, each with a cooperative tone:\n\n1. \"Prioritizes team success with personal benefits as a secondary consideration.\" (10 words)\n2. generated.\n----------generation 0 completed----------\nGeneration 0: Cooperation=0.619, Fitness=2.858\nmutation: Sure! Here are three rephrased descriptions of the person's character, each with a more selfish tone:\n\n1. \"Prioritizes personal interests while considering group outcomes.\" (12 words)\n2. \"Self\n['D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'C', 'C', 'D', 'C', 'C'] for: Sure! Here are three rephrased descriptions of the person's character, each with a more selfish tone:\n\n1. \"Prioritizes personal interests while considering group outcomes.\" (12 words)\n2. \"Self generated.\n----------generation 1 completed----------\nmutation: Sure! Here are three rephrased descriptions of the character's personality, each with a more selfish tone:\n\n1. \"Self-serving team player who prioritizes personal interests.\" (10 words)\n2.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"final_population = results['final_population']\nword_analysis = results['word_analysis']\nprint(\"Final word analysis:\", word_analysis)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}